{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0r_DQG3d20C"
      },
      "source": [
        "# Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-wmQukcUzc2",
        "outputId": "a3025302-09c6-4224-f79f-9dcbb3d927f5"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q mediapipe torch_geometric\n",
        "print(\"Libraries installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F9jMgwDXl5J"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, auc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Klls4SKeBoC"
      },
      "source": [
        "# Mediapipe Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21qCRw8CapRO"
      },
      "outputs": [],
      "source": [
        "!wget -q -O face_landmarker.task https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
        "base_options = python.BaseOptions(model_asset_path='face_landmarker.task')\n",
        "options = vision.FaceLandmarkerOptions(\n",
        "    base_options=base_options,\n",
        "    output_face_blendshapes=False,\n",
        "    output_facial_transformation_matrixes=False,\n",
        "    num_faces=1)\n",
        "detector = vision.FaceLandmarker.create_from_options(options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4o2ArZYa3CJ"
      },
      "outputs": [],
      "source": [
        "# Pre-calculate edges of the graphs\n",
        "connections = list(vision.FaceLandmarksConnections.FACE_LANDMARKS_TESSELATION)\n",
        "src = [c[0] if isinstance(c, tuple) else c.start for c in connections]\n",
        "dst = [c[1] if isinstance(c, tuple) else c.end for c in connections]\n",
        "EDGE_INDEX = torch.tensor([src + dst, dst + src], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yaBkgkqeISq"
      },
      "source": [
        "# Data Preparation for Closed Set task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgESexH4IFoR"
      },
      "outputs": [],
      "source": [
        "# Transformations\n",
        "global_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def process_data_to_dual_stream(images, labels):\n",
        "    processed_data = []\n",
        "\n",
        "    for img_arr, label in tqdm(zip(images, labels), total=len(images)):\n",
        "\n",
        "        img_u8 = (img_arr * 255).astype(np.uint8)\n",
        "\n",
        "        # MediaPipe Detection\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_u8)\n",
        "        detection_result = detector.detect(mp_image)\n",
        "\n",
        "        if detection_result.face_landmarks:\n",
        "            landmarks = detection_result.face_landmarks[0]\n",
        "\n",
        "            # Create Graph Data\n",
        "            # Node Features are JUST coordinates (x, y, z)\n",
        "            pos = torch.tensor([[lm.x, lm.y, lm.z] for lm in landmarks], dtype=torch.float32)\n",
        "            graph = Data(x=pos, edge_index=EDGE_INDEX, pos=pos)\n",
        "\n",
        "            # Image Data\n",
        "            processed_data.append({\n",
        "                'image': img_u8,\n",
        "                'graph': graph,\n",
        "                'label': label\n",
        "            })\n",
        "\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "37b9ff43a699485ca3a89b4b8074d659",
            "5fcca1c744bb4fdc94b901796491e961",
            "0f78c69720634d0c803ab941e9fea741",
            "3c6ddb13a75e4229a28872ca4cc43677",
            "6eab873b59fb43e6839756d70d151bef",
            "a0ba21abcd41420388b806ae57d56d23",
            "53d1f2b229a34f7587efa89b1eaec674",
            "43a71d94c2c74fdaa648e940eaa148e6",
            "1c22599400a34b28b8a9db231222a66d",
            "66436ed237c14938b8f7d329f38c5ef4",
            "c313a956245e4ea2b395a8645a7c7f22"
          ]
        },
        "id": "ttIxV2IFa-fa",
        "outputId": "faab5b43-c344-48cd-f532-de69f735ac69"
      },
      "outputs": [],
      "source": [
        "# Load LFW dataset with people that have at least 3 pictures\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=3, resize=1.0, color=True)\n",
        "dataset_list = process_data_to_dual_stream(lfw_people.images, lfw_people.target)\n",
        "print(f\"{len(dataset_list)} valid face samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN5S5i94IWgO"
      },
      "outputs": [],
      "source": [
        "# Dataset classes for test and evaluation for the open and closed set cases. Output: pairs of images and graphs\n",
        "class DualStreamPairDatasetOpen(Dataset):\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.transform = transform\n",
        "        self.label_to_indices = {}\n",
        "\n",
        "        # Group indices by label\n",
        "        for idx, item in enumerate(data_list):\n",
        "            lbl = item['label']\n",
        "            if lbl not in self.label_to_indices:\n",
        "                self.label_to_indices[lbl] = []\n",
        "            self.label_to_indices[lbl].append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Fetch the first sample\n",
        "        item1 = self.data_list[index]\n",
        "        label1 = item1['label']\n",
        "\n",
        "        img1 = item1['image']\n",
        "        if self.transform: img1 = self.transform(img1)\n",
        "        graph1 = item1['graph'].clone()\n",
        "\n",
        "        # 50% chance of Same Person (0), 50% Different (1)\n",
        "        should_get_same_class = random.randint(0, 1) == 0\n",
        "\n",
        "        if should_get_same_class:\n",
        "            possible_indices = self.label_to_indices[label1]\n",
        "            # If only one image exists for this person, we must pick it\n",
        "            if len(possible_indices) == 1:\n",
        "                idx2 = index\n",
        "            else:\n",
        "                idx2 = index\n",
        "                while idx2 == index: # Find a different image of same person\n",
        "                    idx2 = random.choice(possible_indices)\n",
        "            target = 0.0\n",
        "        else:\n",
        "            # Pick a random different label\n",
        "            all_labels = list(self.label_to_indices.keys())\n",
        "            target_label = random.choice(all_labels)\n",
        "            while target_label == label1:\n",
        "                target_label = random.choice(all_labels)\n",
        "\n",
        "            idx2 = random.choice(self.label_to_indices[target_label])\n",
        "            target = 1.0\n",
        "\n",
        "        # Fetch the second sample\n",
        "        item2 = self.data_list[idx2]\n",
        "        img2 = item2['image']\n",
        "        if self.transform: img2 = self.transform(img2)\n",
        "        graph2 = item2['graph'].clone()\n",
        "\n",
        "        return img1, graph1, img2, graph2, torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "class DualStreamPairDatasetClosed(Dataset):\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.transform = transform\n",
        "        self.label_to_indices = {}\n",
        "\n",
        "        for idx, item in enumerate(data_list):\n",
        "            lbl = item['label']\n",
        "            if lbl not in self.label_to_indices:\n",
        "                self.label_to_indices[lbl] = []\n",
        "            self.label_to_indices[lbl].append(idx)\n",
        "\n",
        "        self.valid_indices = [\n",
        "            idx for idx, item in enumerate(data_list)\n",
        "            if len(self.label_to_indices[item['label']]) > 1\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        index = self.valid_indices[i]\n",
        "\n",
        "        item1 = self.data_list[index]\n",
        "        label1 = item1['label']\n",
        "        img1 = self.transform(item1['image']) if self.transform else item1['image']\n",
        "        graph1 = item1['graph'].clone()\n",
        "\n",
        "        should_get_same_class = random.randint(0, 1) == 0\n",
        "\n",
        "        if should_get_same_class:\n",
        "            possible_indices = self.label_to_indices[label1]\n",
        "            idx2 = index\n",
        "            while idx2 == index:\n",
        "                idx2 = random.choice(possible_indices)\n",
        "            target = 0.0\n",
        "        else:\n",
        "            all_labels = list(self.label_to_indices.keys())\n",
        "            target_label = random.choice(all_labels)\n",
        "            while target_label == label1:\n",
        "                target_label = random.choice(all_labels)\n",
        "            idx2 = random.choice(self.label_to_indices[target_label])\n",
        "            target = 1.0\n",
        "\n",
        "        item2 = self.data_list[idx2]\n",
        "        img2 = self.transform(item2['image']) if self.transform else item2['image']\n",
        "        graph2 = item2['graph'].clone()\n",
        "\n",
        "        return img1, graph1, img2, graph2, torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "# Collate function to manage images and graphs\n",
        "def dual_pair_collate_fn(batch):\n",
        "    imgs1 = torch.stack([item[0] for item in batch])\n",
        "    graphs1 = Batch.from_data_list([item[1] for item in batch])\n",
        "\n",
        "    imgs2 = torch.stack([item[2] for item in batch])\n",
        "    graphs2 = Batch.from_data_list([item[3] for item in batch])\n",
        "\n",
        "    targets = torch.stack([item[4] for item in batch])\n",
        "\n",
        "    return imgs1, graphs1, imgs2, graphs2, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKkW1bPhT0kh"
      },
      "outputs": [],
      "source": [
        "# Dataset class to handle graphs and images\n",
        "class DualStreamDataset(Dataset):\n",
        "    def __init__(self, data_list, transform=None):\n",
        "        self.data_list = data_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "\n",
        "        img = item['image']\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        graph = item['graph'].clone()\n",
        "\n",
        "        label = torch.tensor(item['label'], dtype=torch.long)\n",
        "        return img, graph, label\n",
        "\n",
        "# Collator to handle Graph Batches\n",
        "def dual_collate_fn(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    graphs = Batch.from_data_list([item[1] for item in batch])\n",
        "    labels = torch.stack([item[2] for item in batch])\n",
        "    return images, graphs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ngsg7Lck1v",
        "outputId": "094bb0e1-c49c-4d43-ef49-6c74d76b4134"
      },
      "outputs": [],
      "source": [
        "# Split data for train/val/test\n",
        "train_val_data, test_data = train_test_split(\n",
        "    dataset_list, test_size=0.15, stratify=[d['label'] for d in dataset_list], random_state=42\n",
        ")\n",
        "train_data, val_data = train_test_split(\n",
        "    train_val_data, test_size=(0.15/0.85), stratify=[d['label'] for d in train_val_data], random_state=42\n",
        ")\n",
        "\n",
        "# Training loader (Batches of single images for ArcFace Loss)\n",
        "train_loader = DataLoader(\n",
        "    DualStreamDataset(train_data, transform=global_transform),\n",
        "    batch_size=32, shuffle=True, collate_fn=dual_collate_fn\n",
        ")\n",
        "\n",
        "# Validation and Test loader (Pairs for Verification Metrics)\n",
        "val_loader = DataLoader(\n",
        "    DualStreamPairDatasetClosed(val_data, transform=global_transform),\n",
        "    batch_size=32, shuffle=False, collate_fn=dual_pair_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    DualStreamPairDatasetClosed(test_data, transform=global_transform),\n",
        "    batch_size=32, shuffle=False, collate_fn=dual_pair_collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "all_labels = [d['label'] for d in dataset_list]\n",
        "num_classes = len(set(all_labels))\n",
        "print(f\"Training with {num_classes} unique identities.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UuU8sbJeSzs"
      },
      "source": [
        "# Loss and Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-ZAXn87Ofcj"
      },
      "outputs": [],
      "source": [
        "# Define ArcFace Loss\n",
        "class ArcFaceLoss(nn.Module):\n",
        "    def __init__(self, in_features, num_classes, s=30.0, m=0.40):\n",
        "        super(ArcFaceLoss, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.num_classes = num_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "        # Classifier weight matrix\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, features, targets):\n",
        "        # Normalize features and weights\n",
        "        features = F.normalize(features)\n",
        "        W = F.normalize(self.weight)\n",
        "\n",
        "        # Dot product, i.e. Cosine similarity\n",
        "        cosine = F.linear(features, W)\n",
        "\n",
        "        # Angular Margin\n",
        "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        target_logits = torch.cos(theta + self.m)\n",
        "\n",
        "        # One-Hot encoding for targets\n",
        "        one_hot = torch.zeros(cosine.size(), device=features.device)\n",
        "        one_hot.scatter_(1, targets.view(-1, 1).long(), 1.0)\n",
        "\n",
        "        # Apply margin only to the correct class and scale\n",
        "        output = one_hot * target_logits + (1.0 - one_hot) * cosine\n",
        "        output *= self.s\n",
        "\n",
        "        return F.cross_entropy(output, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du8eOuF0OgB1"
      },
      "outputs": [],
      "source": [
        "class HybridFaceNetworkV2(nn.Module):\n",
        "    def __init__(self, embedding_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # ResNet18\n",
        "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.cnn.fc = nn.Linear(512, embedding_dim)\n",
        "        self.cnn_bn = nn.BatchNorm1d(embedding_dim)\n",
        "\n",
        "        # GNN with GAT\n",
        "        self.gnn_conv1 = GATv2Conv(3, 32, heads=4, concat=True)\n",
        "        self.gnn_bn1 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.gnn_conv2 = GATv2Conv(128, 32, heads=4, concat=True)\n",
        "        self.gnn_bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.gnn_conv3 = GATv2Conv(128, embedding_dim, heads=1, concat=False)\n",
        "        self.gnn_bn3 = nn.BatchNorm1d(embedding_dim)\n",
        "\n",
        "        # Final Layer\n",
        "        self.fusion_fc = nn.Linear(256, 128)\n",
        "\n",
        "    def forward_cnn(self, images):\n",
        "        x = self.cnn(images)\n",
        "        x = self.cnn_bn(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def forward_gnn(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = self.gnn_conv1(x, edge_index)\n",
        "        x = self.gnn_bn1(x)\n",
        "        x = F.elu(x)\n",
        "\n",
        "        x = self.gnn_conv2(x, edge_index)\n",
        "        x = self.gnn_bn2(x)\n",
        "        x = F.elu(x)\n",
        "\n",
        "        x = self.gnn_conv3(x, edge_index)\n",
        "        x = self.gnn_bn3(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return x\n",
        "\n",
        "    def forward(self, images, graph_data):\n",
        "        # Extract features\n",
        "        emb_cnn = self.forward_cnn(images)\n",
        "        emb_gnn = self.forward_gnn(graph_data)\n",
        "\n",
        "        # Concatenate\n",
        "        combined = torch.cat([emb_cnn, emb_gnn], dim=1)\n",
        "\n",
        "        # Final Layer and Normalization\n",
        "        final_emb = self.fusion_fc(combined)\n",
        "        return F.normalize(final_emb, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJYeajCLcp07"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    distances = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img1, g1, img2, g2, targets in loader:\n",
        "            img1, g1 = img1.to(device), g1.to(device)\n",
        "            img2, g2 = img2.to(device), g2.to(device)\n",
        "\n",
        "            emb1 = model(img1, g1)\n",
        "            emb2 = model(img2, g2)\n",
        "\n",
        "            dists = 1 - F.cosine_similarity(emb1, emb2)\n",
        "\n",
        "            distances.extend(dists.cpu().numpy())\n",
        "            labels.extend(targets.numpy())\n",
        "\n",
        "    distances = np.array(distances)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    thresholds = np.arange(0, 2.0, 0.05)\n",
        "    best_acc = 0\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        preds = (distances > thresh).astype(float)\n",
        "\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "\n",
        "    return best_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQEC7ll_cvRN",
        "outputId": "a77fdde9-18d9-442b-9e9e-c89e94216da7"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = HybridFaceNetworkV2(embedding_dim=128).to(device)\n",
        "criterion = ArcFaceLoss(in_features=128, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.parameters()},\n",
        "    {'params': criterion.parameters()}\n",
        "], lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrSjjIyBeaBx"
      },
      "source": [
        "# Training the Model for the Closed Set task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qD4Ab59TNw5",
        "outputId": "c0e5555f-4507-457d-84a6-ef7aa0c46b6a"
      },
      "outputs": [],
      "source": [
        "best_val_acc = 0\n",
        "epochs = 25\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, graphs, labels in train_loader:\n",
        "        imgs, graphs, labels = imgs.to(device), graphs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(imgs, graphs)\n",
        "        loss = criterion(embeddings, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Check accuracy on validation pairs\n",
        "    val_acc = validate(model, val_loader, device)\n",
        "\n",
        "    # Checkpointing\n",
        "    save_msg = \"\"\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"closed_hybrid_model.pth\")\n",
        "        save_msg = \"--> Best Model Saved!\"\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc*100:.2f}% {save_msg}\")\n",
        "\n",
        "print(f\"Final Best Validation Accuracy: {best_val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xp28wZVegeh"
      },
      "source": [
        "# Test the Model and Compute Performance Metrics for the Closed Set task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgbP71gBTQHT"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "distances = []\n",
        "labels_gt = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for img1, g1, img2, g2, targets in test_loader:\n",
        "\n",
        "        img1 = img1.to(device)\n",
        "        g1 = g1.to(device)\n",
        "        img2 = img2.to(device)\n",
        "        g2 = g2.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        emb1 = model(img1, g1)\n",
        "        emb2 = model(img2, g2)\n",
        "\n",
        "        dists = 1 - F.cosine_similarity(emb1, emb2)\n",
        "\n",
        "        distances.extend(dists.cpu().numpy())\n",
        "        labels_gt.extend(targets.cpu().numpy())\n",
        "\n",
        "distances = np.array(distances)\n",
        "labels_gt = np.array(labels_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p2vcud_drwH",
        "outputId": "1f1d5dac-d567-4e11-e8a4-8e887fe7f114"
      },
      "outputs": [],
      "source": [
        "# Metric calculation\n",
        "\n",
        "# Sort distances to create thresholds\n",
        "thresholds = np.sort(distances)\n",
        "\n",
        "fars = []\n",
        "frrs = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred_same = distances < thresh\n",
        "\n",
        "    is_same_person = (labels_gt == 0)\n",
        "    is_diff_person = (labels_gt == 1)\n",
        "\n",
        "    # False Accept Rate (FAR)\n",
        "    num_fa = np.sum(pred_same & is_diff_person)\n",
        "    num_neg = np.sum(is_diff_person)\n",
        "    far = num_fa / num_neg if num_neg > 0 else 0\n",
        "\n",
        "    # False Reject Rate (FRR)\n",
        "    num_fr = np.sum((~pred_same) & is_same_person)\n",
        "    num_pos = np.sum(is_same_person)\n",
        "    frr = num_fr / num_pos if num_pos > 0 else 0\n",
        "\n",
        "    fars.append(far)\n",
        "    frrs.append(frr)\n",
        "\n",
        "fars = np.array(fars)\n",
        "frrs = np.array(frrs)\n",
        "gars = 1 - frrs  # Genuine Accept Rate (GAR)\n",
        "\n",
        "# Equal Error Rate\n",
        "diffs = np.abs(fars - frrs)\n",
        "min_diff_idx = np.argmin(diffs)\n",
        "eer_threshold = thresholds[min_diff_idx]\n",
        "eer_val = (fars[min_diff_idx] + frrs[min_diff_idx]) / 2\n",
        "\n",
        "# AUC\n",
        "sorted_indices = np.argsort(fars)\n",
        "roc_auc = auc(fars[sorted_indices], gars[sorted_indices])\n",
        "\n",
        "print(f\"\\n--- Evaluation Results ---\")\n",
        "print(f\"Best Threshold (EER): {eer_threshold:.4f}\")\n",
        "print(f\"EER (Equal Error Rate): {eer_val:.2f}\")\n",
        "print(f\"AUC (Area Under Curve): {roc_auc:.4f}\")\n",
        "\n",
        "# ML Mentrics at ERR threshold\n",
        "final_preds = (distances > eer_threshold).astype(float)\n",
        "\n",
        "acc = accuracy_score(labels_gt, final_preds)\n",
        "prec = precision_score(labels_gt, final_preds, pos_label=0)\n",
        "rec = recall_score(labels_gt, final_preds, pos_label=0)\n",
        "f1 = f1_score(labels_gt, final_preds, pos_label=0)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "E3JZCcasdsgc",
        "outputId": "849dae5a-a7aa-4556-ac51-94a03c821a4c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Distance Histograms\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(distances[labels_gt==0], bins=30, alpha=0.6, color='green', label='Same Person')\n",
        "plt.hist(distances[labels_gt==1], bins=30, alpha=0.6, color='red', label='Diff Person')\n",
        "plt.axvline(eer_threshold, color='black', linestyle='--', label='EER Threshold')\n",
        "plt.title(\"Cosine Distance Distributions (Hybrid Model)\")\n",
        "plt.xlabel(\"Distance (Lower is more similar)\")\n",
        "plt.legend()\n",
        "\n",
        "# FAR vs FRR\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(thresholds, fars, label='FAR (False Accept)', color='red')\n",
        "plt.plot(thresholds, frrs, label='FRR (False Reject)', color='blue')\n",
        "plt.scatter(eer_threshold, eer_val, color='black', zorder=5)\n",
        "plt.text(eer_threshold, eer_val + 0.05, f\" EER: {eer_val:.2f}\", fontsize=10)\n",
        "plt.title(\"FAR vs FRR Curves\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Error Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "nrapthmRgZOQ",
        "outputId": "646b9c1b-6b31-4b01-a4a1-06df3f1adb05"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# ROC Curve (GAR vs FAR)\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(fars, gars, color='darkorange', lw=2, label=f'ROC curve (EER = {eer_val:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Accept Rate (FAR)')\n",
        "plt.ylabel('Genuine Accept Rate (GAR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# DET Curve (FRR vs FAR in Log scale)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.loglog(fars, frrs, color='blue', lw=2)\n",
        "plt.scatter(eer_val, eer_val, color='black', zorder=5, label=f'EER {eer_val:.2f}')\n",
        "plt.xlabel('False Accept Rate (FAR) - Log Scale')\n",
        "plt.ylabel('False Reject Rate (FRR) - Log Scale')\n",
        "plt.title('Detection Error Tradeoff (DET)')\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "U69afciXdez1",
        "outputId": "b0f3060f-0ffe-419e-ea63-27f674f0b384"
      },
      "outputs": [],
      "source": [
        "# CMC / CMS CALCULATION\n",
        "def calculate_cmc(model, data_list, device, transform, max_rank=50):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    loader = DataLoader(\n",
        "        DualStreamDataset(data_list, transform=transform),\n",
        "        batch_size=32, shuffle=False, collate_fn=dual_collate_fn\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, graphs, labels in loader:\n",
        "            imgs, graphs = imgs.to(device), graphs.to(device)\n",
        "            embeddings = model(imgs, graphs)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Calculate Similarity Matrix, matrix shape: [N_probes, N_gallery]\n",
        "    sim_matrix = torch.mm(all_embeddings, all_embeddings.t())\n",
        "\n",
        "    # Mask self-similarity (diagonal) so we don't match an image with itself\n",
        "    sim_matrix.fill_diagonal_(-1)\n",
        "\n",
        "    num_samples = all_labels.size(0)\n",
        "    ranks = []\n",
        "\n",
        "    # For each probe, find the rank of the first correct match\n",
        "    for i in range(num_samples):\n",
        "        query_label = all_labels[i]\n",
        "        _, sorted_indices = torch.sort(sim_matrix[i], descending=True)\n",
        "\n",
        "        matching_labels = (all_labels[sorted_indices] == query_label).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        if len(matching_labels) > 0:\n",
        "            first_match_rank = matching_labels[0].item() + 1\n",
        "            ranks.append(first_match_rank)\n",
        "\n",
        "    # Calculate CMS at each rank\n",
        "    cms_scores = []\n",
        "    for k in range(1, max_rank + 1):\n",
        "        count = sum(1 for r in ranks if r <= k)\n",
        "        cms_scores.append(count / num_samples)\n",
        "\n",
        "    return cms_scores\n",
        "\n",
        "# Run calculation\n",
        "max_k = 100\n",
        "cmc_curve = calculate_cmc(model, test_data, device, global_transform, max_rank=max_k)\n",
        "\n",
        "print(f\"CMS at Rank 1 (Top-1 Accuracy): {cmc_curve[0]*100:.2f}%\")\n",
        "print(f\"CMS at Rank 5: {cmc_curve[4]*100:.2f}%\")\n",
        "print(f\"CMS at Rank 10: {cmc_curve[9]*100:.2f}%\")\n",
        "\n",
        "# Plot CMC\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, max_k + 1), cmc_curve, marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Rank (k)')\n",
        "plt.ylabel('Probability of Identification (CMS)')\n",
        "plt.title('Cumulative Match Characteristic (CMC) Curve')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim([0, 1.05])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5ePAVSKg50Y"
      },
      "source": [
        "# Data Preparation for Open Set task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_adxvY5f-5_",
        "outputId": "e071eaf4-9fb6-4095-afa2-03bd82e1141c"
      },
      "outputs": [],
      "source": [
        "all_labels = list(set([d['label'] for d in dataset_list]))\n",
        "\n",
        "# Split the identities, not the images\n",
        "train_labels, val_test_labels = train_test_split(\n",
        "    all_labels, train_size=0.70, random_state=42\n",
        ")\n",
        "\n",
        "val_labels, test_labels = train_test_split(\n",
        "    val_test_labels, test_size=0.50, random_state=42\n",
        ")\n",
        "\n",
        "train_data = [d for d in dataset_list if d['label'] in train_labels]\n",
        "val_data   = [d for d in dataset_list if d['label'] in val_labels]\n",
        "test_data  = [d for d in dataset_list if d['label'] in test_labels]\n",
        "\n",
        "print(f\"Open-Set Split -> Train People: {len(train_labels)}, Val People: {len(val_labels)}, Test People: {len(test_labels)}\")\n",
        "print(f\"Total Samples -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "num_classes = len(train_labels)\n",
        "\n",
        "label_map = {old_label: i for i, old_label in enumerate(train_labels)}\n",
        "for d in train_data:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Create Loaders\n",
        "train_loader = DataLoader(\n",
        "    DualStreamDataset(train_data, transform=global_transform),\n",
        "    batch_size=32, shuffle=True, collate_fn=dual_collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    DualStreamPairDatasetOpen(val_data, transform=global_transform),\n",
        "    batch_size=32, shuffle=False, collate_fn=dual_pair_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    DualStreamPairDatasetOpen(test_data, transform=global_transform),\n",
        "    batch_size=32, shuffle=False, collate_fn=dual_pair_collate_fn\n",
        ")\n",
        "\n",
        "all_labels = [d['label'] for d in dataset_list]\n",
        "num_classes = len(set(all_labels))\n",
        "print(f\"Training with {num_classes} unique identities.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ2h1XZFiJyX"
      },
      "source": [
        "# Training the Model for the Open Set task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8PWS7uDhO1D",
        "outputId": "30c51920-f626-4677-8383-c9a57ced40fe"
      },
      "outputs": [],
      "source": [
        "best_val_acc = 0\n",
        "epochs = 25\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, graphs, labels in train_loader:\n",
        "        imgs, graphs, labels = imgs.to(device), graphs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(imgs, graphs)\n",
        "        loss = criterion(embeddings, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Check accuracy on validation pairs\n",
        "    val_acc = validate(model, val_loader, device)\n",
        "\n",
        "    # Checkpointing\n",
        "    save_msg = \"\"\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"open_hybrid_model.pth\")\n",
        "        save_msg = \"--> Best Model Saved!\"\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc*100:.2f}% {save_msg}\")\n",
        "\n",
        "print(f\"Final Best Validation Accuracy: {best_val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq_baDSXjJl8"
      },
      "source": [
        "# Test the Model and Compute Performance Metrics for the Open Set task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l9-sP0mh31S"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "distances = []\n",
        "labels_gt = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for img1, g1, img2, g2, targets in test_loader:\n",
        "\n",
        "        img1 = img1.to(device)\n",
        "        g1 = g1.to(device)\n",
        "        img2 = img2.to(device)\n",
        "        g2 = g2.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        emb1 = model(img1, g1)\n",
        "        emb2 = model(img2, g2)\n",
        "\n",
        "        dists = 1 - F.cosine_similarity(emb1, emb2)\n",
        "\n",
        "        distances.extend(dists.cpu().numpy())\n",
        "        labels_gt.extend(targets.cpu().numpy())\n",
        "\n",
        "distances = np.array(distances)\n",
        "labels_gt = np.array(labels_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQBGLWVdh6sm",
        "outputId": "e810b0fc-f7e9-48c7-ab24-03c5a82f4786"
      },
      "outputs": [],
      "source": [
        "# Metric calculation\n",
        "\n",
        "# Sort distances to create thresholds\n",
        "thresholds = np.sort(distances)\n",
        "\n",
        "fars = []\n",
        "frrs = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    pred_same = distances < thresh\n",
        "\n",
        "    is_same_person = (labels_gt == 0)\n",
        "    is_diff_person = (labels_gt == 1)\n",
        "\n",
        "    # False Accept Rate (FAR)\n",
        "    num_fa = np.sum(pred_same & is_diff_person)\n",
        "    num_neg = np.sum(is_diff_person)\n",
        "    far = num_fa / num_neg if num_neg > 0 else 0\n",
        "\n",
        "    # False Reject Rate (FRR)\n",
        "    num_fr = np.sum((~pred_same) & is_same_person)\n",
        "    num_pos = np.sum(is_same_person)\n",
        "    frr = num_fr / num_pos if num_pos > 0 else 0\n",
        "\n",
        "    fars.append(far)\n",
        "    frrs.append(frr)\n",
        "\n",
        "fars = np.array(fars)\n",
        "frrs = np.array(frrs)\n",
        "gars = 1 - frrs  # Genuine Accept Rate (GAR)\n",
        "\n",
        "# Equal Error Rate\n",
        "diffs = np.abs(fars - frrs)\n",
        "min_diff_idx = np.argmin(diffs)\n",
        "eer_threshold = thresholds[min_diff_idx]\n",
        "eer_val = (fars[min_diff_idx] + frrs[min_diff_idx]) / 2\n",
        "\n",
        "# AUC\n",
        "sorted_indices = np.argsort(fars)\n",
        "roc_auc = auc(fars[sorted_indices], gars[sorted_indices])\n",
        "\n",
        "print(f\"\\n--- Evaluation Results ---\")\n",
        "print(f\"Best Threshold (EER): {eer_threshold:.4f}\")\n",
        "print(f\"EER (Equal Error Rate): {eer_val:.4f}%\")\n",
        "print(f\"AUC (Area Under Curve): {roc_auc:.4f}\")\n",
        "\n",
        "# ML Mentrics at ERR threshold\n",
        "final_preds = (distances > eer_threshold).astype(float)\n",
        "\n",
        "acc = accuracy_score(labels_gt, final_preds)\n",
        "prec = precision_score(labels_gt, final_preds, pos_label=0)\n",
        "rec = recall_score(labels_gt, final_preds, pos_label=0)\n",
        "f1 = f1_score(labels_gt, final_preds, pos_label=0)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}%\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "H2jLvF8Lh8QL",
        "outputId": "c472af83-a010-452d-f855-28248290eda5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Distance Histograms\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(distances[labels_gt==0], bins=30, alpha=0.6, color='green', label='Same Person')\n",
        "plt.hist(distances[labels_gt==1], bins=30, alpha=0.6, color='red', label='Diff Person')\n",
        "plt.axvline(eer_threshold, color='black', linestyle='--', label='EER Threshold')\n",
        "plt.title(\"Cosine Distance Distributions (Hybrid Model)\")\n",
        "plt.xlabel(\"Distance (Lower is more similar)\")\n",
        "plt.legend()\n",
        "\n",
        "# FAR vs FRR\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(thresholds, fars, label='FAR (False Accept)', color='red')\n",
        "plt.plot(thresholds, frrs, label='FRR (False Reject)', color='blue')\n",
        "plt.scatter(eer_threshold, eer_val, color='black', zorder=5)\n",
        "plt.text(eer_threshold, eer_val + 0.05, f\" EER: {eer_val:.2f}\", fontsize=10)\n",
        "plt.title(\"FAR vs FRR Curves\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Error Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "gnR164uJh9v9",
        "outputId": "1440691a-b7d3-423a-a61b-f90ae027dabd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# ROC Curve (GAR vs FAR)\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(fars, gars, color='darkorange', lw=2, label=f'ROC curve (EER = {eer_val:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Accept Rate (FAR)')\n",
        "plt.ylabel('Genuine Accept Rate (GAR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# DET Curve (FRR vs FAR in Log scale)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.loglog(fars, frrs, color='blue', lw=2)\n",
        "plt.scatter(eer_val, eer_val, color='black', zorder=5, label=f'EER {eer_val:.2f}')\n",
        "plt.xlabel('False Accept Rate (FAR) - Log Scale')\n",
        "plt.ylabel('False Reject Rate (FRR) - Log Scale')\n",
        "plt.title('Detection Error Tradeoff (DET)')\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "a4qe2ia1h-9c",
        "outputId": "de65c14e-e9d8-4d54-a49e-1238d20f6152"
      },
      "outputs": [],
      "source": [
        "# CMC / CMS CALCULATION\n",
        "def calculate_cmc(model, data_list, device, transform, max_rank=50):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    loader = DataLoader(\n",
        "        DualStreamDataset(data_list, transform=transform),\n",
        "        batch_size=32, shuffle=False, collate_fn=dual_collate_fn\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, graphs, labels in loader:\n",
        "            imgs, graphs = imgs.to(device), graphs.to(device)\n",
        "            embeddings = model(imgs, graphs)\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Calculate Similarity Matrix, matrix shape: [N_probes, N_gallery]\n",
        "    sim_matrix = torch.mm(all_embeddings, all_embeddings.t())\n",
        "\n",
        "    # Mask self-similarity (diagonal) so we don't match an image with itself\n",
        "    sim_matrix.fill_diagonal_(-1)\n",
        "\n",
        "    num_samples = all_labels.size(0)\n",
        "    ranks = []\n",
        "\n",
        "    # For each probe, find the rank of the first correct match\n",
        "    for i in range(num_samples):\n",
        "        query_label = all_labels[i]\n",
        "        _, sorted_indices = torch.sort(sim_matrix[i], descending=True)\n",
        "\n",
        "        matching_labels = (all_labels[sorted_indices] == query_label).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        if len(matching_labels) > 0:\n",
        "            first_match_rank = matching_labels[0].item() + 1\n",
        "            ranks.append(first_match_rank)\n",
        "\n",
        "    # Calculate CMS at each rank\n",
        "    cms_scores = []\n",
        "    for k in range(1, max_rank + 1):\n",
        "        count = sum(1 for r in ranks if r <= k)\n",
        "        cms_scores.append(count / num_samples)\n",
        "\n",
        "    return cms_scores\n",
        "\n",
        "# Run calculation\n",
        "max_k = 100\n",
        "cmc_curve = calculate_cmc(model, test_data, device, global_transform, max_rank=max_k)\n",
        "\n",
        "print(f\"CMS at Rank 1 (Top-1 Accuracy): {cmc_curve[0]*100:.2f}%\")\n",
        "print(f\"CMS at Rank 5: {cmc_curve[4]*100:.2f}%\")\n",
        "print(f\"CMS at Rank 10: {cmc_curve[9]*100:.2f}%\")\n",
        "\n",
        "# Plot CMC\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, max_k + 1), cmc_curve, marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Rank (k)')\n",
        "plt.ylabel('Probability of Identification (CMS)')\n",
        "plt.title('Cumulative Match Characteristic (CMC) Curve')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim([0, 1.05])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f78c69720634d0c803ab941e9fea741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a71d94c2c74fdaa648e940eaa148e6",
            "max": 7606,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c22599400a34b28b8a9db231222a66d",
            "value": 7606
          }
        },
        "1c22599400a34b28b8a9db231222a66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37b9ff43a699485ca3a89b4b8074d659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fcca1c744bb4fdc94b901796491e961",
              "IPY_MODEL_0f78c69720634d0c803ab941e9fea741",
              "IPY_MODEL_3c6ddb13a75e4229a28872ca4cc43677"
            ],
            "layout": "IPY_MODEL_6eab873b59fb43e6839756d70d151bef"
          }
        },
        "3c6ddb13a75e4229a28872ca4cc43677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66436ed237c14938b8f7d329f38c5ef4",
            "placeholder": "",
            "style": "IPY_MODEL_c313a956245e4ea2b395a8645a7c7f22",
            "value": "7606/7606[02:29&lt;00:00,57.36it/s]"
          }
        },
        "43a71d94c2c74fdaa648e940eaa148e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d1f2b229a34f7587efa89b1eaec674": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fcca1c744bb4fdc94b901796491e961": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0ba21abcd41420388b806ae57d56d23",
            "placeholder": "",
            "style": "IPY_MODEL_53d1f2b229a34f7587efa89b1eaec674",
            "value": "100%"
          }
        },
        "66436ed237c14938b8f7d329f38c5ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eab873b59fb43e6839756d70d151bef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ba21abcd41420388b806ae57d56d23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c313a956245e4ea2b395a8645a7c7f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
